{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/stephanelassalvy/logistic-regression-pima-s-diabetes-data?scriptVersionId=120272065\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","execution_count":1,"id":"b301fe2c","metadata":{"_execution_state":"idle","_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","execution":{"iopub.execute_input":"2023-02-25T11:18:34.430754Z","iopub.status.busy":"2023-02-25T11:18:34.428273Z","iopub.status.idle":"2023-02-25T11:18:34.558866Z","shell.execute_reply":"2023-02-25T11:18:34.557179Z"},"papermill":{"duration":0.139802,"end_time":"2023-02-25T11:18:34.561982","exception":false,"start_time":"2023-02-25T11:18:34.42218","status":"completed"},"tags":[]},"outputs":[],"source":["# logisticRegressionAddons_20230217.R : addons to assess a logistic regression fit\n","# Pima_Diabetes_Logistic_Regression_20230215.Rmd : example using a dataset from Kaggle\n","\n","#    The 2 R scripts are under GPL3 Licence.\n","\n","# The dataset is available on Kaggle under CC0 licence :\n","# License : CC0: Public Domain. Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press, and it is published t to reuse in the google research dataset.\n","\n","# Link/URL for getting the data :\n","# https://www.kaggle.com/datasets/whenamancodes/predict-diabities\n","\n","# Link/URL for the .R and .Rnw scripts :\n","# https://github.com/stephaneLabs/Pimas_Diabetes_LogisticRegression"]},{"cell_type":"code","execution_count":2,"id":"5fc4f313","metadata":{"execution":{"iopub.execute_input":"2023-02-25T11:18:34.605439Z","iopub.status.busy":"2023-02-25T11:18:34.570041Z","iopub.status.idle":"2023-02-25T11:18:37.306717Z","shell.execute_reply":"2023-02-25T11:18:37.304993Z"},"papermill":{"duration":2.74499,"end_time":"2023-02-25T11:18:37.309444","exception":false,"start_time":"2023-02-25T11:18:34.564454","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["ResourceSelection 0.3-5 \t 2019-07-22\n","\n","Loading required package: Matrix\n","\n","\n","Attaching package: ‘arules’\n","\n","\n","The following objects are masked from ‘package:base’:\n","\n","    abbreviate, write\n","\n","\n","Loading required package: carData\n","\n","\n","Attaching package: ‘car’\n","\n","\n","The following object is masked from ‘package:arules’:\n","\n","    recode\n","\n","\n","The following object is masked from ‘package:DescTools’:\n","\n","    Recode\n","\n","\n","Registered S3 method overwritten by 'GGally':\n","  method from   \n","  +.gg   ggplot2\n","\n"]}],"source":["#' Tools for Logit regression\n","#'\n","#' Copyright Stéphane Lassalvy 2023 02 17\n","#' \n","#' Licence GPL-3\n","#' Disclaimer of Warranty.\n","#'\n","#' THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. \n","#' EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES\n","#' PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED,\n","#' INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\n","#' FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.\n","#' SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING,\n","#' REPAIR OR CORRECTION.\n","#' \n","#' \n","#' \n","library(ggplot2)\n","library(ggpubr)\n","library(DescTools)\n","library(ResourceSelection)\n","library(moments)\n","library(arules)\n","library(car)\n","library(GGally)\n","\n","#' Function : quantitativeVariableDescription\n","#' Histogram of a quantitative variable, with skewness and kurtosis information\n","#' Inputs :\n","#' @param df data frame including the variable of interest\n","#' @param variableName name of the variable of interest\n","#' \n","#' Outputs :\n","#' Descriptive graphics for the variable of interest\n","#' \n","quantitativeVariableDescription <- function(df, variableName){\n","  \n","  is_outlier <- function(x) {\n","    return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))\n","  }\n","  \n","  # Extract the variable of interest\n","  extractVariable <- function(df, variableName) {variable <- eval(parse(text = paste(\"df$\", variableName, sep=\"\")));\n","                                                 cat(\"Lenght of the variable :\\n\")\n","                                                 cat(length(variable))\n","                                                 cat(\"\\n\")\n","                                                 cat(\"Variable summary :\\n\")\n","                                                 print(summary(variable))\n","                                                 return(variable)\n","                                                }\n","\n","  variable <- extractVariable(df, variableName)\n","\n","  # Data frame from the variable of interest only, NA removed\n","  df_Graphics <- data.frame(variable = variable)\n","  df_Graphics <- subset(df_Graphics, !is.na(variable))\n","  df_Graphics <- mutate(df_Graphics, outlier = is_outlier(variable))\n","\n","  # Graphics\n","  figureHistogram <- ggplot(df_Graphics, aes(x = variable)) + \n","                            geom_histogram(aes(y = after_stat(density)), colour = 1, fill = \"lightblue\") +\n","                            geom_density(color=\"darkgreen\") +\n","                            xlab(paste(\"Values, Skewness = \", round(skewness(df_Graphics$variable),2), \"Kurstosis = \", round(kurtosis(df_Graphics$variable), 2))) +\n","                            ylab(\"Prob.\") +\n","                            ggtitle(paste(\"Histogram of \", toupper(variableName), sep = \"\")) +\n","                            geom_vline(xintercept = mean(df_Graphics$variable), color = \"blue\", linewidth = 1) +\n","                            geom_vline(xintercept = median(df_Graphics$variable), color = \"darkgreen\", linewidth = 1)  +\n","                            stat_function(fun = dnorm, args = list(mean = mean(df_Graphics$variable), sd = sd(df_Graphics$variable)), col = \"blue\", linewidth = 1)\n","\n","  figureBoxplot <- ggplot(df_Graphics, aes(x=eval(variableName), y=variable)) + \n","                          geom_boxplot(outlier.size=2, outlier.colour=\"black\") + \n","                          xlab(toupper(variableName)) + ylab(paste(\"Values of \", toupper(variableName), sep=\"\")) +\n","                          geom_text(data = subset(df_Graphics, outlier == TRUE), aes(label = variable), na.rm = TRUE, hjust = -0.3) + \n","                          ggtitle(paste(\"Boxplot and outliers of \", toupper(variableName), sep=\"\"))\n","  \n","  figure <- ggarrange(figureHistogram, figureBoxplot, nrow = 1, ncol = 2)\n","  figure\n","}\n","\n","\n","#' Function : removeOutliers\n","#' Removes the rows of a dataframe having outliers for a given variable\n","#' Inputs :\n","#' @param df data frame including the variable of interest\n","#' @param variableName name of the variable of interest\n","#' \n","#' Outputs :\n","#' \n","#' a list with the initial dataframe and the cleaned dataframe\n","#' \n","removeOutliers <- function(df, variableName =\"name\"){\n","  #find Q1, Q3, and interquartile range for values in column A\n","  cat(\"Number of rows of initial dataframe : \")\n","  cat(nrow(df))\n","  cat(\"\\n\")\n","  \n","  variable <- eval(parse(text = paste(\"df$\",variableName,sep=\"\"))) \n","  Q1 <- quantile(variable, .25)\n","  Q3 <- quantile(variable, .75)\n","  IQR <- IQR(variable)\n","  \n","  #only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3\n","  df_without_outliers <- subset(df, variable > (Q1 - 1.5*IQR) & variable < (Q3 + 1.5*IQR))\n","  \n","  cat(\"Number of rows of toiletted dataframe : \")\n","  cat(nrow(df_without_outliers))\n","  cat(\"\\n\")\n","  \n","  cat(\"Percent of rows deleted : \")\n","  cat(paste(round(nrow(df_without_outliers) / nrow(df), 2), \"%\"))\n","  cat(\"\\n\")\n","  return(list(initial_df = df, without_df = df_without_outliers))\n","}\n","\n","\n","#' Function : logOddsVsQuantitativePredictor\n","#' Relationship between the log odds and the quantitative predictor\n","#' Inputs :\n","#' @param df data frame including the variable of interest\n","#' @param binaryResponse name of the binary response\"\n","#' @param quantitativePredictor name of the quantitative predictor\n","#' @param method method to discretize the quantitative predictor (\"round\" rounds the quantitative variable to \"digits\" digits,\n","#'        \"interval\" (equal interval width), \"frequency\" (equal frequency), \"cluster\" (k-means clustering) \n","#'         and \"fixed\" (categories specifies interval boundaries).\n","#'         The function then use the rounded variable (\"round\" option) or compute the mean for each level of the discretization.\n","#' @param digits number of digits for the quantitative predictor\n","#' @param breaks option for the discretize function from package arules\n","#' @param variableName name of the variable of interest\n","#' \n","#' Outputs :\n","#' Descriptive graphics for the variable of interest\n","#' \n","logOddsVsQuantitativePredictor <- function(df, binaryResponse = \"dm\", quantitativePredictor, method = \"round\", digits = 2, breaks = 3){\n","  # Get the variables of interest\n","  binaryResp     <- as.factor(eval(parse(text=paste(\"df$\", binaryResponse, sep = \"\"))))\n","  xQuantitative  <- as.numeric(eval(parse(text=paste(\"df$\", quantitativePredictor, sep = \"\"))))\n","  \n","  # Discretize the quantitative predictor to observe the relationship between means of predictor values and log(odds) of the response\n","  if(method==\"round\"){\n","    quantPredictor  <- as.factor(round(xQuantitative, digits = digits))\n","  } else {\n","    xDiscretization <- as.factor(as.character(discretize(xQuantitative, method = method, breaks = breaks)))\n","    ddf <- data.frame(xQuantitative = xQuantitative, xDiscretization  = xDiscretization)\n","    quantPredictor <- aggregate(xQuantitative ~ xDiscretization, data = ddf, FUN = \"mean\", na.action = \"na.omit\")\n","    quantPredictor <- rename(quantPredictor, Xmeans = xQuantitative)\n","    ddf            <- full_join(ddf, quantPredictor, by = \"xDiscretization\")\n","    quantPredictor <- as.numeric(ddf$Xmeans)\n","  }\n","  if(any(binaryResp == \"\")){binaryResp[binaryResp == \"\"] <- NA}\n","  if(any(quantPredictor == \"\")){quantPredictor[quantPredictor == \"\"] <- NA}\n","\n","  # Compute the odds for each value of predictor\n","  frequencyTable <- prop.table(table(quantPredictor , binaryResp), margin = 1)\n","  odds <- frequencyTable[, \"1\"] / frequencyTable[, \"0\"]\n","\n","  # Make a table with log(odds) and predictor\n","  oddsTable <- data.frame(predictor = as.numeric(row.names(frequencyTable)),\n","                          \"_1\" = frequencyTable[, \"1\"],\n","                          \"_0\" = frequencyTable[, \"0\"],\n","                          odds = odds,\n","                          logodds = log(odds))\n","\n","  \n","  oddsTable <- oddsTable[oddsTable$logodds != -Inf & oddsTable$logodds != Inf, ]\n","  \n","  # regression and correlations between log(odds) and predictor\n","  coefOddsModel <- coef(lm(logodds ~ predictor, data = oddsTable))\n","  correlationPredictorVsLogOdds <- with(oddsTable, round(cor(predictor, odds, use = \"complete.obs\"), 3))\n","\n","  # print relationship between log(odds) and predictor\n","  if(method==\"round\"){\n","    with(oddsTable, {figure <- ggplot(data = oddsTable, aes(x = predictor, y = logodds)) +\n","      geom_point() +\n","      geom_abline(intercept = coefOddsModel[1], slope = coefOddsModel[2], linetype = 2, color = \"red\") +\n","      xlab(paste(\"Predictor : \", toupper(quantitativePredictor), \", digits = \", digits)) +\n","      ylab(\"Log(Odds)\") +\n","      ggtitle(paste(\"Predictor :\", toupper(quantitativePredictor),\" vs Log(Odds),\\n corr = \", correlationPredictorVsLogOdds))\n","    figure\n","    })\n","  } else {\n","    with(oddsTable, {figure <- ggplot(data = oddsTable, aes(x = predictor, y = logodds)) +\n","                               geom_point() +\n","                               geom_abline(intercept = coefOddsModel[1], slope = coefOddsModel[2], linetype = 2, color = \"red\") +\n","                               xlab(paste(\"Predictor : \", toupper(quantitativePredictor), \", means of \", nrow(oddsTable), \" groups\")) +\n","                               ylab(\"Log(Odds)\") +\n","                               ggtitle(paste(\"Predictor :\", toupper(quantitativePredictor),\" vs Log(Odds),\\n corr = \", correlationPredictorVsLogOdds))\n","                     figure\n","    })\n","  }\n","}\n","\n","\n","#' Function : binaryResponseVSCategoricalPredictor\n","#' Percentage table and conditionnal percentage tables crossing the binary response with a categorical variable\n","#' Inputs :\n","#' @param binaryResponse : name of the binary response variable\n","#' @param categoricalPredictor : name of the categorical predictor\n","#' @param digits : number of digits in the text output\n","#' \n","#' Output :\n","#' Only text outputs\n","#' \n","binaryResponseVSCategoricalPredictor <- function(df, binaryResponse = \"dm\", categoricalPredictor, digits = 2){\n","  # Cross table of the two categorical variables\n","  binaryResp <- as.character(eval(parse(text=paste(\"df$\", binaryResponse, sep = \"\"))))\n","  categoPred <- as.character(eval(parse(text=paste(\"df$\", categoricalPredictor, sep = \"\"))))\n","  \n","  binaryResp[binaryResp == \"\"] <- NA\n","  categoPred[categoPred == \"\"] <- NA\n","  \n","  binaryResp <- as.factor(binaryResp)\n","  categoPred <- as.factor(categoPred)\n","  \n","  # Discard lines with missing data\n","  df <- na.omit(data.frame(binaryResp, categoPred))\n","\n","  responseVsPredictorFrequencies <- with(df, table(binaryResp, categoPred))\n","  \n","  # Proportion of observation in each cell of the table\n","  responseVsPredictorProportions <- round(addmargins(100*prop.table(responseVsPredictorFrequencies)), digits = digits)\n","  \n","  # Chi-square test of independance based on the observed frequencies\n","  responseVsPredictorChi2 <- chisq.test(responseVsPredictorFrequencies)\n","  \n","  # Conditional proportions \n","  responseVsPredictorConditionnalProportion1 <- round(addmargins(100*prop.table(responseVsPredictorFrequencies, margin = 1), margin = 2), digits = digits)\n","  responseVsPredictorConditionnalProportion2 <- round(addmargins(100*prop.table(responseVsPredictorFrequencies, margin = 2), margin = 1), digits = digits)\n","  \n","  return(invisible(list(frequencies = responseVsPredictorFrequencies,\n","                        proportions = responseVsPredictorProportions,\n","                        independanceChi2 = responseVsPredictorChi2,\n","                        ConditionnalProportions1 = responseVsPredictorConditionnalProportion1,\n","                        ConditionnalProportions2 = responseVsPredictorConditionnalProportion2\n","                        )\n","                   )\n","         )\n","}\n","\n","\n","#' Function plotProportionsResponseConditionnedByPredictor\n","#' Plot stacked barplot of proportions of binary outcome conditioned by the qualitative predictor levels\n","#' Inputs :\n","#' @param tableConditionnalProportions2 conditioned proportions of type 2 given by the function binaryResponseVSCategoricalPredictor\n","#' \n","#' Outputs :\n","#' Stacked bar plot of the conditioned proportions\n","#' \n","plotProportionsResponseConditionnedByPredictor <- function(tableConditionnalProportions2, VariableName = \"Variable\"){\n","  df_graph <- as.data.frame(tableConditionnalProportions2[1:2,])\n","  figure <- ggplot(df_graph, aes(x = categoPred, y = Freq, fill = binaryResp)) +\n","            geom_bar(position = \"stack\", stat = \"identity\") +\n","            ylab(\"Proportions in %\") +\n","            xlab(VariableName) +\n","            scale_fill_discrete(name=\"Response\") +\n","            ggtitle(paste(\"Proportion of each group of the response within each levels of \", VariableName))\n","  print(figure)\n","}\n","\n","\n","#' plotVIF\n","#' Plot Variable Inflation Factor for variables in a linear of logit model to assess multicolinearity among the predictors\n","#' Inputs :\n","#' @param model the considered model\n","#' \n","#' Outputs :\n","#' Diagram of the VIF value withe 3 thresholds as a rule of thumb : \n","#' 2.5 warning, 5 some concerns, 10 serious problem\n","#' \n","plotVIF <- function(model){\n","  vifDf <- as.data.frame(vif(model))\n","  vifDf <- rownames_to_column(vifDf, var = \"rowName\")\n","  barplotFigure <- ggplot(data = vifDf, aes(x = rowName, y = GVIF)) +\n","                   geom_col(color =\"blue\", fill = \"lightblue\") +\n","                   xlab(\"Variable/Factor Predictor\") +\n","                   ylab(\"GVIF value\") +\n","                   ggtitle(\"VIF Values for the considered model\") +\n","                   geom_hline(yintercept = c(2.5, 5, 10), color = c(\"darkgreen\", \"blue\", \"red\")) +\n","                   theme(axis.text.x=element_text(angle  = 90, vjust = 0.5, hjust = 1))\n","  print(barplotFigure)\n","  return(vifDf)\n","}\n","\n","\n","#' Function mcFaddenR2\n","#' Inputs :\n","#' @param model : current model M\n","#' @param nullmondel : null model M0\n","#' \n","#' Output :\n","#' Mc Fadden R2\n","#'  \n","mcFaddenR2 <- function(model, nullModel){\n","  R2 <- as.numeric(1 - logLik(model) / logLik(nullModel))\n","  return(R2)\n","}\n","\n","\n","#' Function adjMcFaddenR2\n","#' Inputs :\n","#' @param model : current model M\n","#' @param nullmondel : null model M0\n","#' \n","#' Output :\n","#' R2 of Mc Fadden value \n","#' \n","adjMcFaddenR2 <- function(model, nullModel){\n","  R2 <-  as.numeric(1 - (logLik(model) - length(coef(model))) / (logLik(nullModel) - 1))\n","  return(R2)\n","}\n","\n","\n","#' Function sasR2\n","#' Inputs :\n","#' @param model : current model M\n","#' @param nullmondel : null model M0\n","#' \n","#' Output :\n","#' SAS R2\n","#' \n","sasR2 <- function(model, nullModel){n  <- nrow(na.omit(model$data))\n","                                    R2 <- as.numeric(1 - exp(-2 * (logLik(model) - logLik(nullModel)) / n))\n","                                    return(R2)\n","}\n","\n","#' Function adjSasR2\n","#' Inputs :\n","#' @param model : current model M\n","#' @param nullmondel : null model M0\n","#' \n","#' Output :\n","#' Ajusted SAS R2\n","#' \n","adjSasR2 <- function(model, nullModel){\n","                                       n  <- nrow(na.omit(model$data))\n","                                       R2 <- as.numeric(1 - exp(-2 * (logLik(model) - logLik(nullModel)) / n))\n","                                       adjR2 <- as.numeric(R2 / (1 - exp(2 * logLik(nullModel) / n)))\n","                                       return(adjR2)\n","}\n","\n","#' Function devR2\n","#' Inputs :\n","#' @param model : current model M\n","#' @param nullmondel : null model M0\n","#' \n","#' Output :\n","#' dev R2\n","#' \n","devR2 <- function(model, nullModel){\n","  n <- nrow(na.omit(model$data))\n","  nullDeviance  <- model$null.deviance\n","  modelDeviance <- model$deviance\n","  deltaDeviance <- nullDeviance - modelDeviance\n","  # it's strange that 2*(logL(M) - logL(M0)) does not give the same delta-deviance\n","  R2 <- deltaDeviance / nullDeviance\n","  return(R2)\n","}\n","\n","\n","#' Function R2s\n","#' Sum up the different R2 measure in a data frame\n","#' \n","#' Inputs :\n","#' @param model : the model to be assessed\n","#' @param nullModel : the null model, having the Intercept as the only predictor\n","#' \n","#' Output:\n","#' result : data frame with the different outputs\n","#' \n","computeR2s <- function(model, nullModel){\n","  results <- data.frame(mcFadden_R2    = mcFaddenR2(model, nullModel),\n","                        adjMcFadden_R2 = adjMcFaddenR2(model, nullModel),\n","                        sas_R2         = sasR2(model, nullModel),\n","                        adjSas_R2      = adjSasR2(model, nullModel),\n","                        dev_R2         = devR2(model, nullModel)\n","  )\n","}\n","\n","\n","#' Function HoslemTest\n","#' Performs the Hoslem Lemeshow test and the related charts\n","#' Inputs :\n","#' @param model   : the current model M\n","#' @param ngroups : number of groups to perform de test\n","#' \n","#' Outputs :\n","#' Print the results of the test and displays the related graphics \n","#' \n","HoslemTest <- function(model, ngroups = 10){\n","  # Compute and display Hoslem Ledeshow test results\n","  hoslemTest <- hoslem.test(x = model$y, y = fitted(model), g = ngroups)\n","  print(hoslemTest)\n","\n","  df <- as_tibble(cbind(hoslemTest$observed, hoslemTest$expected))\n","  \n","  # Display the related graphics\n","  # Observed cases vs Predicted cases for each of the 10 groups\n","  observed1VsPredicted1 <- ggplot(data = df, aes(x = y1, y = yhat1))  +\n","                           geom_point() +\n","                           geom_abline(intercept = 0, slope = 1, color=\"red\") +\n","                           geom_smooth(method =lm, formula = y ~ x, color = \"blue\") +\n","                           xlab(\"Observed cases\") + \n","                           ylab(\"Predicted cases\") +\n","                           ggtitle(\"Obs. cases Vs Pred. cases\")\n","\n","  # print(observed1VsPredicted1)\n","  observed0VsPredicted0 <- ggplot(data = df, aes(x = y0, y = yhat0))  +\n","                           geom_point() +\n","                           geom_abline(intercept = 0, slope = 1, color=\"red\") +\n","                           geom_smooth(method =lm, formula = y ~ x, color = \"blue\") +\n","                           xlab(\"Observed cases\") + \n","                           ylab(\"Predicted cases\") +\n","                           ggtitle(\"Obs. non cases Vs Pred. non cases\")\n","  \n","  df <- df %>% mutate(total = y1 + y0, \n","                      totalexpected = yhat1 + yhat0,\n","                      y1pct = y1 / total * 100,\n","                      yhat1pct= yhat1 / totalexpected * 100)\n","  \n","  # Observed cases vs Predicted cases for each of the 10 groups in %\n","  observed1VsPredicted1pct <- ggplot(data = df, aes(x = y1pct, y = yhat1pct))  +\n","                              geom_point() +\n","                              geom_abline(intercept = 0, slope = 1, color=\"red\") +\n","                              geom_smooth(method =lm, formula = y ~ x, color = \"blue\") +\n","                              xlab(\"Observed cases\") + \n","                              ylab(\"Predicted cases\") +\n","                              ggtitle(\"Obs. cases Vs Pred. cases in %\")\n","  \n","  outputGraph <- ggarrange(observed1VsPredicted1, observed0VsPredicted0, observed1VsPredicted1, nrow = 1, ncol = 3)\n","  print(outputGraph)\n","  }\n","\n","#' Residuals plots\n","#' \n","residualPlots <- function(model, binaryresponse = \"DiabeteDiagostic\", quantitativePredictor = \"none\", label.size = 3){\n"," df <- model$data\n"," df$response <- as.factor(eval(parse(text = paste(\"df$\", binaryresponse))))\n"," \n"," df$cooksDistances    <- cooks.distance(model)\n"," df$cooksIndex        <- seq(along.with = as.numeric(df$cooksDistances))\n"," \n"," df$leverages        <- influence(model)$hat\n"," df$indexLeverages   <- seq(along.with = as.numeric(df$leverages))\n"," \n"," df$pearsonResiduals  <- residuals(model, type = \"pearson\")\n"," df$pearsonIndex      <- seq(along.with = as.numeric(df$pearsonResiduals))\n"," \n"," df$devianceResiduals <- residuals(model, type = \"deviance\")\n"," df$devianceIndex     <- seq(along.with = as.numeric(df$devianceResiduals))\n","\n"," df$devianceResiduals <- residuals(model, type = \"deviance\")\n"," df$devianceIndex     <- seq(along.with = as.numeric(df$devianceResiduals))\n","\n"," df <- df %>% mutate(stdPearsonResiduals  = pearsonResiduals  / sqrt(1 - leverages),\n","                     stdDevianceResiduals = devianceResiduals / sqrt(1 - leverages))\n"," \n"," if(quantitativePredictor == \"none\"){\n","   # Cook's distances figure\n","   cooksTreshold <- 4 / nrow(df)\n","   CooksFigure  <- ggplot(data = df, aes(ymax = cooksDistances, ymin = 0, x=cooksIndex, color = response)) +\n","                          geom_linerange() +\n","                          geom_text(data = subset(df, cooksDistances > cooksTreshold),\n","                                    aes(y = cooksDistances, x=cooksIndex, label = cooksIndex), \n","                                    hjust = -0.1, vjust = 0.1, size = label.size, check_overlap = TRUE, color = \"black\") +\n","                          geom_hline(yintercept = cooksTreshold, color = \"red\") +\n","                          xlab(\"Observation index\") +\n","                          ylab(\"Cook's distances\") +\n","                          ggtitle(\"Cook's distances\") +\n","                          scale_colour_discrete(name = \"Response\")\n","   \n","   # Pearson residuals figure\n","   pearsonResFigure  <- ggplot(data = df, aes(y = pearsonResiduals, x=pearsonIndex, color = response)) +\n","                        geom_point() +\n","                        geom_text(data = subset(df, cooksDistances > cooksTreshold), \n","                                  aes(y = pearsonResiduals, x=pearsonIndex, label = pearsonIndex),\n","                                  hjust = -0.1, vjust = 0.1, size = label.size, check_overlap = TRUE, color = \"black\") +\n","                        geom_hline(yintercept = 0, color = \"black\") +\n","                        xlab(\"Observation index\") +\n","                        ylab(\"Pearson residuals\") +\n","                        ggtitle(\"Pearson residuals\") +\n","                        scale_colour_discrete(name = \"Response\") \n","   \n","   # Pearson standardized residuals figure\n","   stdPearsonResFigure  <- ggplot(data = df, aes(y = stdPearsonResiduals, x=pearsonIndex, color = response)) +\n","                           geom_point() +\n","                           geom_text(data = subset(df, cooksDistances > cooksTreshold), \n","                                     aes(y = stdPearsonResiduals, x=pearsonIndex, label = pearsonIndex), \n","                                     hjust = -0.1, vjust = 0.1, size = label.size, check_overlap = TRUE, color = \"black\") +\n","                           geom_hline(yintercept = 0, color = \"black\") +\n","                           xlab(\"Observation index\") +\n","                           ylab(\"Standardized Pearson residuals\") +\n","                           ggtitle(\"Standardized Pearson residuals\") +\n","                           scale_colour_discrete(name = \"Response\")\n","   \n","   # Leverages figures\n","   nParameters <- sum(df$leverages)\n","   leveragesTreshold <- 3 * nParameters / nrow(df)\n","   leveragesFigure   <- ggplot(data = df, aes(ymax = leverages, ymin = 0, x=indexLeverages, color = response)) +\n","                        geom_linerange() +\n","                        geom_hline(yintercept = leveragesTreshold, color = \"red\") +\n","                        geom_text(data = subset(df, leverages > leveragesTreshold), \n","                                  aes(y = leverages, x=indexLeverages, label = indexLeverages), \n","                                  hjust = -0.1, vjust = 0.1, size = label.size, check_overlap = TRUE, color = \"black\") +\n","                        xlab(\"Observation index\") +\n","                        ylab(\"Leverages\") +\n","                        ggtitle(\"Leverages\")  +\n","                        scale_colour_discrete(name = \"Response\")\n","   \n","     \n","   # Deviance residuals figure\n","   devianceResFigure <- ggplot(data = df, aes(y = devianceResiduals, x=devianceIndex, color = response)) +\n","                        geom_point() +\n","                        geom_text(data = subset(df, cooksDistances > cooksTreshold), \n","                                  aes(y = devianceResiduals, x=devianceIndex, label = devianceIndex), \n","                                  hjust = -0.1, vjust = 0.1, size = label.size, check_overlap = TRUE, color = \"black\") +\n","                        geom_hline(yintercept = 0, color = \"black\") +\n","                        xlab(\"Observation index\") +\n","                        ylab(\"Deviance residuals\") +\n","                        ggtitle(\"Deviance residuals\") +\n","                        scale_colour_discrete(name = \"Response\")       \n","   \n","   # Deviance standardized residuals figure\n","   stdDevianceResFigure <- ggplot(data = df, aes(y = stdDevianceResiduals, x=devianceIndex, color = response)) +\n","                           geom_point() +\n","                           geom_text(data = subset(df, cooksDistances > cooksTreshold), \n","                                     aes(y = stdDevianceResiduals, x=devianceIndex, label = devianceIndex), \n","                                     hjust = -0.1, vjust = 0.1, size = label.size, check_overlap = TRUE, color = \"black\") +\n","                           geom_hline(yintercept = 0, color = \"black\") +\n","                           xlab(\"Observation index\") +\n","                           ylab(\"Standardized Deviance residuals\") +\n","                           ggtitle(\"Standardized Deviance residuals\")  +\n","                           scale_colour_discrete(name = \"Response\")\n","   \n","   figure <- ggarrange(CooksFigure, pearsonResFigure, stdPearsonResFigure, leveragesFigure, devianceResFigure, stdDevianceResFigure, nrow = 2, ncol = 3)\n","   figure\n"," }\n","}\n","\n","#' Function plotOddsRatios\n","#'\n","#' Inputs :\n","#' @param oddsRatiosDf odds ratios dataframe with confidence intervals boundaries\n","#' \n","#' Outputs : \n","#' Display Odds Ratios with confidence intervals figure\n","#' \n","plotOddsRatios <- function(oddsRatiosDf){\n","  figure <- ggplot(oddsRatiosDf, aes(x = factor(rowname), y = oddsRatios, ymin = Lower2.5pct, ymax = Upper97.5pct)) +\n","    geom_pointrange() + geom_hline(yintercept = 1, color = \"red\", ) +\n","    ggtitle(\"Odds ratios and confidence intervals\") + xlab(\"Variables/Factor Predictor\") + ylab(\"Odds Ratios\") +\n","    theme(axis.text.x=element_text(angle  = 90, vjust = 0.5, hjust = 1))\n","  print(figure)\n","}\n","\n","\n","#' Function rocCurve\n","#' Inputs\n","#' @param trainedModel   : model trained on the trained data frame\n","#' @param testDf         : test dataset to assess the predictive ability of the model\n","#' @param optimizeMethod : \"Ones\" or \"Zeros\" or \"Both\" or \"misclasserror\" see InvormationValue::optimalCutoff documentation\n","#' Outputs :\n","#' Graphics of ROC curves and misclassification error rate\n","#' \n","rocCurve <- function(trainedModel, testDf = NULL, outcomeVariable = \"Outcome\", optimizeMethod = \"misclasserror\"){\n","  df_train <- trainedModel$data\n","\n","  observedOutcome <- eval(parse(text = paste(\"df_train$\", outcomeVariable, sep = \"\")))\n","  predictedProbs  <- predict(m_train, type = \"response\")\n","    \n","  \n","  #' Function sensibilityAndSpecificity\n","  #' \n","  #' Inputs \n","  #' @param observedOutcome observed binary outcome 1 = \"yes\"\n","  #' @param predictedProbs  predicted probabilities for binary outcome being 1 = \"yes\"\n","  #' \n","  #' Outputs\n","  #' \n","  #' @return result a data frame containing False predictive rate, True predictive rate and Misclassification error rate\n","  sensibilityAndSpecificity <- function(observedOutcome, predictedProbs, threshold){\n","    # Compute predicted binary outcome\n","    predictedOutcome  <- as.factor(ifelse(predictedProbs > threshold, 1, 0))\n","    if(all(levels(predictedOutcome)==\"0\")){levels(predictedOutcome) <- c(\"0\",\"1\")}\n","    if(all(levels(predictedOutcome)==\"1\")){levels(predictedOutcome) <- c(\"0\",\"1\")}\n","    \n","    # Confusion matrix between observed and predicted binary outcome\n","    conf_matrix <- table(predictedOutcome, observedOutcome)\n","    \n","    # False predictive rate, True predictive rate and Misclassification error rate\n","    truePredictiveRate  <- caret::sensitivity(conf_matrix, reference = factor(c(1,0)), positive = factor(c(1,0))[1])\n","    falsePredictiveRate <- 1 - caret::specificity(conf_matrix, reference = factor(c(1,0)), negative = factor(c(1,0))[2])\n","    misClassErrRate     <- (conf_matrix[1,2] + conf_matrix[2,1]) / sum(conf_matrix)\n","    result              <- data.frame(FPR = falsePredictiveRate, TPR = truePredictiveRate, MCER = misClassErrRate, threshold = threshold) \n","    return(result)\n","  }\n","\n","  \n","  #'Function distance to bisectrix\n","  #'\n","  #' Inputs :\n","  #' @matrix x, matrix with coordinates x = FPR and y = TPR\n","  distancesToBisectrix <- function(matFprTpr){\n","    \n","    distances <- -as.matrix(matFprTpr) %*% c(1, -1) / sqrt(2)\n","    rankMaximum <- which.max(distances)\n","    \n","    if(length(rankMaximum) > 1){\n","      warning(\"several distances correspond to the maximal distance between the ROC and the bisectrix, we choose the minimum of this ranks\")\n","      rankMaximum <- min(rankMaximum)\n","    } else {\n","    }\n","    \n","    distances <- as.matrix(distances, ncol = 1)\n","    colnames(distances) <- \"distanceToBisectrix\"\n","    \n","    return(list(distances = distances, rankMax = rankMaximum))\n","  }\n","  \n","  \n","  # Tresholds for ROC curves (for both training data set and testing data set)\n","  thresholds <- seq(0.00, 1.00, by = 0.01)\n","\n","  # Results for training data set\n","  results_train.list <- lapply(X = thresholds, FUN = sensibilityAndSpecificity, observedOutcome = observedOutcome, predictedProbs = predictedProbs)\n","  results_train      <- do.call(what = rbind, args = results_train.list)\n","  fiftPct_train      <- results_train[results_train$threshold == 0.5,]\n","  \n","  # Look for the point of the train ROC with maximum distance to the bisectrix\n","  maxDistRocToBisectrix_train <- distancesToBisectrix(select(results_train, c(\"FPR\", \"TPR\")))\n","  geom_cutoff_train <- results_train[maxDistRocToBisectrix_train[[\"rankMax\"]],]\n","  \n","  # Include distances to bisectrix in the results\n","  results_train$distToBissec <- maxDistRocToBisectrix_train[[\"distances\"]]\n","  results_train$type         <- \"train\"\n","  \n","  # Optimal cutoff threshold calculated with optimalCutoff\n","  # Notice that optimalCutoff can make the job of the function sensibilityAndSpecificity and give a dataframe with cutoffs, FPR, TPR,\n","  # misclassificationError and Youden's index (option \"Both\") by using the option returnDiagnostics = TRUE\n","  # Optimizasing cutoff with Youden's index seems very close to the geometric method\n","  # This function could really be simplified taking advantage of the optimalCutoff function\n","  optimal_cutoff_train <- optimalCutoff(actuals = observedOutcome, predictedScores = predictedProbs, optimiseFor = optimizeMethod)\n","\n","  # Approximately optimal cutoff line in the results for train data (cutoff rounded to 2 decimals)\n","  optimal_cutoff_result_train <- results_train[results_train$threshold == round(optimal_cutoff_train, 2),]\n","  \n","  # ROC Curve\n","  rocCurve <- ggplot(data = results_train, aes(x = FPR, y = TPR)) +\n","              geom_line(color = \"#009E73\") +\n","              geom_abline(slope = 1, intercept = 0, color =\"blue\") +\n","              geom_point(data = fiftPct_train, aes(x = FPR, y = TPR), color = \"red\") +\n","              geom_text(data = fiftPct_train, aes(x = FPR, y = TPR, label = paste(\"s = \", threshold)), nudge_y = 0.025, color = \"red\") +\n","              geom_text(aes(x = 0.4, y = 0.5, label = paste(\"AUC (train) = \", round(AUROC(observedOutcome, predictedProbs), 4))), color = \"#009E73\") +\n","              geom_vline(xintercept = geom_cutoff_train$FPR, color = \"#009E73\") +\n","              geom_point(data = geom_cutoff_train, aes(x = FPR, y = TPR), color = \"red\") +\n","              geom_text(data = geom_cutoff_train, aes(x = FPR, y = TPR, label = paste(\"Geom. s = \", round(threshold,2))), nudge_y = 0.025, color = \"red\") +\n","              geom_vline(xintercept = optimal_cutoff_result_train$FPR, color = \"#009E73\", linetype = 2) +\n","              geom_point(data = optimal_cutoff_result_train, aes(x = FPR, y = TPR), color = \"red\") +\n","              geom_text(data = optimal_cutoff_result_train, aes(x = FPR, y = TPR, label = paste(\"Optim. s = \", round(threshold,2))), nudge_y = -0.025, color = \"red\") +\n","              xlab(\"False Positive Rate = 1 - Specificity\") + ylab(\"True Positive Rate = Sensitivity\") + ggtitle(paste(\"ROC Curve, optimization of cutoff with option\", optimizeMethod))\n","  \n","  # Misclassification Error curve\n","  MisClassifErrCurve <- ggplot(data = results_train, aes(x = threshold, y = MCER)) +\n","                        geom_line(color = \"#009E73\") +\n","                        geom_vline(xintercept = optimal_cutoff_train, color = \"#009E73\") +\n","                        xlab(\"Thresholds\") + ylab(\"Misclassification error rate\") + ggtitle(\"Misclassification error rate\")\n","  \n","  # Results for testing data set if there is a testing data set\n","  if(!is.null(testDf) == TRUE){\n","    df_test  <- testDf\n","    testObservedOutcome <- eval(parse(text = paste(\"df_test$\", outcomeVariable, sep = \"\")))\n","    \n","    # we keep the same model fit, and only change the data\n","    testPredictedProbs  <- predict(m_train, newdata = df_test, type = \"response\")\n","    results_test.list   <- lapply(X = thresholds, FUN = sensibilityAndSpecificity, observedOutcome = testObservedOutcome, predictedProbs = testPredictedProbs)\n","    results_test        <- do.call(what = rbind, args = results_test.list)\n","    fiftPct_test        <- results_test[results_test$threshold == 0.5,]\n","    \n","    # Look for the point of the train ROC with maximum distance to the bisectrix\n","    maxDistRocToBisectrix_test <- distancesToBisectrix(select(results_test, c(\"FPR\", \"TPR\")))\n","    geom_cutoff_test <- results_test[maxDistRocToBisectrix_test[[\"rankMax\"]],]\n","\n","    # Include distances to bisectrix in the results\n","    results_test$distToBissec <- maxDistRocToBisectrix_test[[\"distances\"]]\n","    results_test$type  <- \"test\"\n","    \n","    # Optimal cutoff threshold calculated with optimalCutoff\n","    optimal_cutoff_test <- optimalCutoff(actuals = testObservedOutcome, testPredictedProbs, optimiseFor = optimizeMethod)\n","\n","    # Approximately optimal cutoff line in the results for test data (cutoff rounded to 2 decimals)\n","    optimal_cutoff_result_test <- results_test[results_test$threshold == round(optimal_cutoff_test, 2),]\n","    \n","    # ROC Curve\n","    rocCurve <- rocCurve +\n","                geom_line(data = results_test, aes(x = FPR, y = TPR), color = \"#E69F00\") +\n","                geom_point(data = fiftPct_test, aes(x = FPR, y = TPR), color = \"red\") +\n","                geom_text(data = fiftPct_test, aes(x = FPR, y = TPR, label = paste(\"s = \", threshold)), nudge_y = 0.025, color = \"red\") +\n","                geom_text(aes(x = 0.4, y = 0.25, label = paste(\"AUC (TEST) = \", round(AUROC(testObservedOutcome, testPredictedProbs), 4))), color = \"#E69F00\") +\n","                geom_vline(xintercept = geom_cutoff_test$FPR, color = \"#E69F00\") +\n","                geom_point(data = geom_cutoff_test, aes(x = FPR, y = TPR), color = \"red\") +\n","                geom_text(data = geom_cutoff_test, aes(x = FPR, y = TPR, label = paste(\"Geom. s = \", round(threshold,2))), nudge_y = 0.025, color = \"red\") +\n","                geom_vline(xintercept = optimal_cutoff_result_test$FPR, color = \"#E69F00\", linetype = 2) +\n","                geom_point(data = optimal_cutoff_result_test, aes(x = FPR, y = TPR), color = \"red\") +\n","                geom_text(data = optimal_cutoff_result_test, aes(x = FPR, y = TPR, label = paste(\"Optim. s = \", round(threshold,2))), nudge_y = -0.025, color = \"red\")\n","    \n","    # Misclassification Error curve\n","    MisClassifErrCurve <- MisClassifErrCurve +\n","                          geom_line(data = results_test, aes(x = threshold, y = MCER), color = \"#E69F00\") +\n","                          geom_vline(xintercept = optimal_cutoff_test, color = \"#E69F00\")\n","      \n","    results       <- rbind(results_train, results_test)\n","\n","    final_result <- list(results = results, geom_cutoff_train = geom_cutoff_train,  optimal_cutoff_train = optimal_cutoff_train, geom_cutoff_test = geom_cutoff_test, optimal_cutoff_test = optimal_cutoff_test)      \n","  } else {\n","    results <- results_train\n","    final_result <- list(results = results, geom_cutoff_train = geom_cutoff_train,  optimal_cutoff_train = optimal_cutoff_train)      \n","    \n","  }\n","  \n","  # Compose and display final plot\n","  finalPlot <- ggarrange(rocCurve, MisClassifErrCurve, nrow = 1, ncol = 2)\n","  print(finalPlot)\n","  \n","  # Return final results\n","  return(invisible(final_result))\n","}\n"]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"4.0.5"},"papermill":{"default_parameters":{},"duration":6.207019,"end_time":"2023-02-25T11:18:37.433819","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-02-25T11:18:31.2268","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}